{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b166a331-a79d-4b4b-80cd-a911f6c132f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"NOTES: Batch data is different each time in keras, which result in slight differences in results.\"\"\"\n",
    "import pickle\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Dropout, MaxPooling1D, Reshape, multiply, Conv1D, GlobalAveragePooling1D, Dense,LSTM\n",
    "# from keras.models import Input, Model, load_model\n",
    "from keras.models import  Model, load_model\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler , EarlyStopping\n",
    "from scipy.interpolate import splev, splrep\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0af0b5cc-c752-446d-af4e-437fb60c1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./apnea-ecg-database-1.0.0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "ir = 3  # interpolate interval\n",
    "before = 2\n",
    "after = 2\n",
    "# normalize\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e381887-a0a3-442e-975d-5f41a984ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, path), 'rb') as f:  # read preprocessing result\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1  = [] \n",
    "    \n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    \n",
    "    #print(\"len(o_train): \",len(o_train))\n",
    "    \n",
    "    for i in range(len(o_train)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_train[i]\n",
    "        # Curve interpolation\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
    "        x_train1.append([rri_interp_signal, ampl_interp_signal])  # 5-minute-long segment\n",
    "       \n",
    "        \n",
    "    x_training1,y_training,groups_training = [],[],[]\n",
    "    x_val1,y_val,groups_val = [],[],[]\n",
    "\n",
    "    # trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))    \n",
    "    with open(os.path.join('./train_list_data', 'trainlist.pkl'), 'rb') as f:  # read preprocessing result\n",
    "        \n",
    "        trainlist = pickle.load(f)\n",
    "\n",
    "    #random.shuffle(trainlist)\n",
    "\n",
    "    num = [i for i in range(len(o_train))]\n",
    "    #trainlist = num[100:]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "        \n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    #print('y_val: ', y_val)\n",
    "    \n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "    x_test1 = []\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "\n",
    "    for i in range(len(o_test)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_test[i]\n",
    "        # Curve interpolation\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
    "        x_test1.append([rri_interp_signal, ampl_interp_signal])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, y_training, groups_training, x_val1, y_val, groups_val, x_test1, y_test, groups_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9c66f0-dcf3-4f46-bd6d-a9bc2a15495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 30 and (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    #print(\"Learning rate: \", lr)\n",
    "    return lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b19045-dedf-4179-920c-5d66a2bb5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, GlobalAveragePooling1D, Dense, Reshape, multiply, Input, Dropout, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def multi_head_self_attention(x, num_heads=4, key_dim=32):\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(0.1)(attn_output)\n",
    "    out = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    return out\n",
    "\n",
    "def create_model(input_a_shape, weight=1e-3):\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=l2(weight))(input1)\n",
    "    x1 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=l2(weight))(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Conv1D(32, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=l2(weight))(x1)\n",
    "    x1 = MaxPooling1D(pool_size=5, padding=\"same\")(x1)\n",
    "    \n",
    "    x2 = Conv1D(16, kernel_size=15, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=l2(weight))(input1)\n",
    "    x2 = Conv1D(24, kernel_size=15, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=l2(weight))(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3, padding=\"same\")(x2)\n",
    "    x2 = Conv1D(32, kernel_size=15, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=l2(weight))(x2)\n",
    "    x2 = MaxPooling1D(pool_size=5, padding=\"same\")(x2)\n",
    "    \n",
    "    concat = keras.layers.concatenate([x1, x2], name=\"Concat_Layer\", axis=-1)\n",
    "    \n",
    "    # LAFM\n",
    "    concat = LSTM(64, return_sequences=True)(concat)\n",
    "    concat = multi_head_self_attention(concat, num_heads=4, key_dim=32)\n",
    "\n",
    "    # Channel Attention\n",
    "    squeeze = GlobalAveragePooling1D()(concat)\n",
    "    excitation = Dense(32, activation='relu')(squeeze)\n",
    "    excitation = Dense(64, activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 64))(excitation)\n",
    "    scale = multiply([concat, excitation])\n",
    "    \n",
    "    scale = LSTM(32, return_sequences=True)(scale)\n",
    "    scale = multi_head_self_attention(scale, num_heads=4, key_dim=32)  # Second MHSA Layer\n",
    "    \n",
    "    # Channel Attention\n",
    "    squeeze = GlobalAveragePooling1D()(scale)\n",
    "    excitation = Dense(16, activation='relu')(squeeze)\n",
    "    excitation = Dense(32, activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 32))(excitation)\n",
    "    scale = multiply([scale, excitation])\n",
    "    \n",
    "    scale = LSTM(32, return_sequences=True)(scale)\n",
    "    scale = multi_head_self_attention(scale, num_heads=4, key_dim=32)  # Third MHSA Layer\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(scale)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(x)\n",
    "    \n",
    "    model = Model(inputs=[input1], outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "001ccf64-3de6-4846-95fd-2cdd5fd1ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "path = \"apnea-ecg.pkl\"\n",
    "x_train1, y_train, groups_train, x_val1, y_val, groups_val, x_test1, y_test, groups_test = load_data(path)    \n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "#print('input_shape', x_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4088055a-9c56-4d5c-8e60-6c07eff86090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train1 length:  11696\n",
      "x_val1 length:  5013\n",
      "x_test1 length:  16945\n"
     ]
    }
   ],
   "source": [
    "print('x_train1 length: ', len(x_train1))\n",
    "print('x_val1 length: ', len(x_val1))\n",
    "print('x_test1 length: ', len(x_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8b3ef3-d33e-47bf-8d60-59166fc91167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 900, 2)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 900, 16)      368         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 900, 16)      496         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 900, 16)      176         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 450, 24)      4248        ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 450, 24)      5784        ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 450, 24)      1944        ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 150, 24)      0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 150, 24)     0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPooling1D)  (None, 150, 24)     0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 150, 32)      8480        ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 150, 32)      11552       ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 150, 32)      3872        ['max_pooling1d_4[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 30, 32)      0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 30, 32)      0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPooling1D)  (None, 30, 32)      0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " Concat_Layer (Concatenate)     (None, 30, 96)       0           ['max_pooling1d_1[0][0]',        \n",
      "                                                                  'max_pooling1d_3[0][0]',        \n",
      "                                                                  'max_pooling1d_5[0][0]']        \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 30, 64)       41216       ['Concat_Layer[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 30, 64)      33216       ['lstm[0][0]',                   \n",
      " dAttention)                                                      'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 30, 64)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 30, 64)      0           ['lstm[0][0]',                   \n",
      " da)                                                              'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 30, 64)      128         ['tf.__operators__.add[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 64)          0           ['layer_normalization[0][0]']    \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           2080        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           2112        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1, 64)        0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 30, 64)       0           ['layer_normalization[0][0]',    \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 30, 32)       12416       ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 30, 32)      16800       ['lstm_1[0][0]',                 \n",
      " eadAttention)                                                    'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 30, 32)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 30, 32)      0           ['lstm_1[0][0]',                 \n",
      " mbda)                                                            'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 30, 32)      64          ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 32)          0           ['layer_normalization_1[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           528         ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 32)           544         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1, 32)        0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)          (None, 30, 32)       0           ['layer_normalization_1[0][0]',  \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 30, 32)       8320        ['multiply_1[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 30, 32)      16800       ['lstm_2[0][0]',                 \n",
      " eadAttention)                                                    'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 30, 32)       0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 30, 32)      0           ['lstm_2[0][0]',                 \n",
      " mbda)                                                            'dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 30, 32)      64          ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2 (Gl  (None, 32)          0           ['layer_normalization_2[0][0]']  \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " Output_Layer (Dense)           (None, 2)            66          ['global_average_pooling1d_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 171,274\n",
      "Trainable params: 171,274\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "i = 0\n",
      "Elapsed time: 199.9 seconds\n",
      "Best model is saved in i = 0 th run with accuracy = 0.8954263794629684\n",
      "i = 1\n",
      "Elapsed time: 386.3 seconds\n",
      "Best model is saved in i = 1 th run with accuracy = 0.8974328710534081\n",
      "i = 2\n",
      "Elapsed time: 575.5 seconds\n",
      "i = 3\n",
      "Elapsed time: 763.1 seconds\n",
      "Best model is saved in i = 3 th run with accuracy = 0.9024491000295072\n",
      "i = 4\n",
      "Elapsed time: 953.0 seconds\n",
      "i = 5\n",
      "Elapsed time: 1142.6 seconds\n",
      "i = 6\n",
      "Elapsed time: 1328.9 seconds\n",
      "i = 7\n",
      "Elapsed time: 1516.0 seconds\n",
      "i = 8\n",
      "Elapsed time: 1704.4 seconds\n",
      "i = 9\n",
      "Elapsed time: 1897.2 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training\n",
    "#me\n",
    "callback_EarlyStopp = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Record the start time\n",
    "start_time = time.time()\n",
    "\n",
    "BestAcc = 0\n",
    "N = 10\n",
    "Result = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    "    model = create_model(x_train1.shape[1:])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if i==0:\n",
    "        model.summary()\n",
    "    \n",
    "    print(f'i = {i}')\n",
    "    \n",
    "    filepath = 'weights.best.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    # me  +callback\n",
    "    #callbacks_list = [lr_scheduler, checkpoint]\n",
    "    callbacks_list = [lr_scheduler, checkpoint,callback_EarlyStopp]\n",
    "    history = model.fit([x_train1], y_train, batch_size=128, epochs=200, verbose=0,\n",
    "                        validation_data=([x_val1], y_val), callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "    # test\n",
    "    #filepath = model_fn\n",
    "    filepath = './weights.best.hdf5'\n",
    "    model = load_model(filepath)\n",
    "    #model = load_model(model_fn)\n",
    "\n",
    "    loss, accuracy = model.evaluate([x_test1], y_test, verbose=0)\n",
    "    # save prediction score\n",
    "    y_score = model.predict([x_test1],  verbose=0)\n",
    "    output = pd.DataFrame({\"y_true\": y_test[:, 1], \"y_score\": y_score[:, 1], \"subject\": groups_test})\n",
    "    output.to_csv(\"./utils/code_for_calculating_per-recording/output/SE-MSCNN.csv\", index=False)\n",
    "    y_true, y_pred = np.argmax(y_test, axis=-1), np.argmax(model.predict([x_test1], batch_size=1024, verbose=0), axis=-1)\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    #print(\"i: {}, acc: {}, sn: {}, sp: {}, f1: {}\".format(i, acc, sn, sp, f1))\n",
    "    Result[i] = acc\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Print the elapsed time\n",
    "    print(f\"Elapsed time: {elapsed_time:.1f} seconds\")\n",
    "\n",
    "    filepath = 'Best_model022.hdf5'\n",
    "    if BestAcc < acc:\n",
    "        BestAcc = acc    \n",
    "        Best_history = history\n",
    "        Best_i = i\n",
    "        model.save(filepath)\n",
    "        print(f'Best model is saved in i = {i} th run with accuracy = {BestAcc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ca653e-dc74-4c99-8318-78cce32a0b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "530/530 [==============================] - 7s 11ms/step - loss: 0.3469 - accuracy: 0.9148\n",
      "530/530 [==============================] - 5s 9ms/step\n",
      "17/17 [==============================] - 1s 37ms/step\n",
      "acc: 0.9147831218648569, sn: 0.8775038520801233, sp: 0.93792443806791, f1: 0.8874863643447094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = 'Best_model022_2_good.hdf5'\n",
    "model = load_model(filepath)\n",
    "\n",
    "loss, accuracy = model.evaluate([x_test1], y_test)\n",
    "# save prediction score\n",
    "y_score = model.predict([x_test1])\n",
    "output = pd.DataFrame({\"y_true\": y_test[:, 1], \"y_score\": y_score[:, 1], \"subject\": groups_test})\n",
    "output.to_csv(\"./utils/code_for_calculating_per-recording/output/SE-MSCNN.csv\", index=False)\n",
    "y_true, y_pred = np.argmax(y_test, axis=-1), np.argmax(model.predict([x_test1], batch_size=1024, verbose=1), axis=-1)\n",
    "C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "print(\"acc: {}, sn: {}, sp: {}, f1: {}\".format(acc, sn, sp, f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc6a8a-3f69-4414-876e-9996a0c6ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print('Average of all Runs = ', np.average(Result))\n",
    "    print('Result = ', Result)\n",
    "\n",
    "    Best_history = history\n",
    "    \n",
    "    print(\"max accuracy = \",max(history.history['accuracy']))    \n",
    "    print(\"max val_accuracy = \",max(history.history['val_accuracy']))\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(history.history['accuracy'] ,color='red')\n",
    "    plt.plot(history.history['val_accuracy'],color='blue')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'] ,color='red')\n",
    "    plt.plot(history.history['val_loss'],color='blue')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd405be4-6fda-4e96-91b6-b133cbf7228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import netron\n",
    "\n",
    "#filepath = 'Best_model022_2_good_9147.hdf5'\n",
    "## Open the model in Netron\n",
    "#netron.start(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31684cf-4ed1-4cc7-8b3f-8d4253beec9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
